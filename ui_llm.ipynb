{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import fire\n",
    "import json\n",
    "from typing import List, Union\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import transformers\n",
    "from transformers import TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    set_peft_model_state_dict\n",
    ")\n",
    "from peft import PeftModel\n",
    "\n",
    "device = 'auto'\n",
    "base_LLM_model = 'meta-llama/Llama-2-7b-hf'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f72aefdf007e45848643f0a434f122dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 9-1. 훈련된 LoRA layer와 base LLM 병합(merge)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_LLM_model,\n",
    "    #load_in_8bit=True, # LoRA\n",
    "    #load_in_4bit=True, # Quantization Load\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cpu\")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, './output/checkpoint-761_a', device)\n",
    "model = model.merge_and_unload().to(\"cuda\")  # Merge!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "              base_LLM_model,\n",
    "              trust_remote_code=True,\n",
    "              eos_token = '</s>' \n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(element):\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    outputs = tokenizer(\n",
    "        element['input'],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_length=True,\n",
    "        padding=True\n",
    "    )\n",
    "    input_batch = []\n",
    "    for inputs, input_ids, labels in zip(element[\"input\"], outputs[\"input_ids\"], element['label']):\n",
    "        input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is this fake news? article: Ronaldo Joins Al Nassr: A New Star in Saudi answer: False'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\\\n",
    "Is this fake news? article: Ronaldo Joins Al Nassr: A New Star in Saudi answer:\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs.to(\"cuda\")\n",
    "\n",
    "# Generate\n",
    "generate_ids = model.generate(inputs.input_ids, max_length = 27)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import scrolledtext, messagebox\n",
    "import random\n",
    "def check_fake_news():\n",
    "    article = entry_var.get().strip()  # 공백 제거하고 텍스트 가져오기\n",
    "    if not article:\n",
    "        messagebox.showinfo(\"Input Required\", \"Please enter an article title to check.\")\n",
    "        return\n",
    "\n",
    "    chat_history.config(state=tk.NORMAL)\n",
    "    chat_history.insert(tk.END, f\"You: {article}\\n\", \"user\")\n",
    "    chat_history.see(tk.END)\n",
    "    prompt_format1 = \"\"\"Determine if the given article is fake. article:\"\"\"\n",
    "    prompt_format2 = \"\"\"Is this article fake? article:\"\"\"\n",
    "    prompt_format3 = \"\"\"Return True if the given article is fake. article:\"\"\"\n",
    "    \n",
    "    prompt_templates = [prompt_format1,prompt_format2,prompt_format3]\n",
    "    chosen_prompt = prompt_templates[random.randint(0, len(prompt_templates)-1)]\n",
    "    prompt = f'{chosen_prompt} article: {article} answer:'\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "    try:\n",
    "        generate_ids = model.generate(inputs.input_ids, max_length=len(inputs.input_ids[0])+1)\n",
    "        output = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "        output_list = output.split()\n",
    "        \n",
    "        output = output_list[-1]\n",
    "        chat_history.insert(tk.END, f\"Bot: It's {output} news.\\n\\n\", \"bot\")\n",
    "        chat_history.see(tk.END)\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", str(e))\n",
    "    \n",
    "    entry_var.set(\"\")  # 입력 필드 초기화\n",
    "    chat_history.config(state=tk.DISABLED)\n",
    "\n",
    "# UI 구성\n",
    "root = tk.Tk()\n",
    "root.title(\"Fake News Checker Chat\")\n",
    "\n",
    "# 채팅 히스토리 영역\n",
    "chat_history = scrolledtext.ScrolledText(root, font=(\"Arial\", 12), wrap=tk.WORD, state=tk.DISABLED, bg=\"#F0F0F0\", width=60, height=20)\n",
    "chat_history.tag_configure(\"user\", foreground=\"#0084FF\")\n",
    "chat_history.tag_configure(\"bot\", foreground=\"#FF5733\")\n",
    "chat_history.pack(padx=20, pady=10)\n",
    "\n",
    "# 입력 필드\n",
    "entry_var = tk.StringVar()\n",
    "article_entry = tk.Entry(root, textvariable=entry_var, font=(\"Arial\", 14), width=53)\n",
    "article_entry.pack(side=tk.LEFT, padx=(20, 0), pady=10)\n",
    "article_entry.bind(\"<Return>\", lambda event: check_fake_news())  # Enter 키를 누르면 실행\n",
    "\n",
    "# 검사 버튼\n",
    "check_button = tk.Button(root, text=\"Send\", command=check_fake_news, font=(\"Arial\", 14), bg=\"#4CAF50\", fg=\"white\")\n",
    "check_button.pack(side=tk.RIGHT, padx=(0, 20), pady=10)\n",
    "\n",
    "root.mainloop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
